max_seq_length: 2048
peft_adapters:
  max_seq_length: ${max_seq_length}
  dtype: None
  load_in_4bit: True
  
model_name: "unsloth/Llama-3.2-3B-Instruct" 
chat_template: "llama-3.1"
sft_configs:
  max_seq_length : ${max_seq_length}
  dataset_text_field: "text"
  dataset_num_proc: 2
  packing: False
peft_configs:
  r: 16
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",]
  lora_alpha: 16
  lora_dropout: 0
  bias: "none"
  use_gradient_checkpointing: "unsloth"
  random_state: 3407
  use_rslora: False
  loftq_config: None
  report_to: "mlflow"
dataset:
  name: "mlabonne/FineTome-100k"
  split: "train[:100]"
