{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset_name = \"hotpotqa/hotpot_qa\"\n",
    "directory = \"hotpotqa_hotpot_qa\"\n",
    "# data_dir = \"distractor\"\n",
    "\n",
    "\n",
    "directory = \"rajpurkar_squad_v2\"\n",
    "hf_dataset_name = \"rajpurkar/squad_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LakeFSCredentials(endpoint_url='https://lakefs.lmorbits.com', access_key_id='AKIAJKZNY4J56OMMLN2Q', secret_access_key='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', namespace='gs://slmops-dev-data-instructed')\n",
      "Repository qa-manual does not exist, creating it now.\n",
      "Created new repo qa-manual using storage namespace gs://slmops-dev-data-instructed/lakefs/qa-manual\n",
      "Found existing branch main\n"
     ]
    }
   ],
   "source": [
    "from data.etl.hf_to_lakefs import stream_and_upload_from_hf_to_lakefs,get_info\n",
    "from data.utils.lakefs import LakeFSCredentials\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "load_dotenv(Path(\".\").resolve().parent / \"secrets\" / \".env\")\n",
    "\n",
    "credentials = LakeFSCredentials.from_env()\n",
    "\n",
    "project_name = \"qa-manual\"\n",
    "dataset_type = \"raw\"\n",
    "# directory = \"bookcorpus-test-3\"\n",
    "branch_name = \"main\"\n",
    "\n",
    "from data.utils.lakefs import LakeFsDataset,DatasetType\n",
    "lakefs_dataset = LakeFsDataset(credentials=credentials,\n",
    "                               dataset_type=DatasetType(\"raw\"), \n",
    "                               directory=directory, \n",
    "                               project_name=project_name, \n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Streaming from Hugging Face: 1851it [00:01, 1658.56it/s]/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/lmorbits/.venv/lib/python3.12/site-packages/dask_expr/_collection.py:302: UserWarning: Dask annotations {'retries': 5} detected. Annotations will be ignored when using query-planning.\n",
      "  warnings.warn(\n",
      "\u001b[32m2025-02-13 13:28:03.324\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 0 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_0\u001b[0m\n",
      "Streaming from Hugging Face: 2940it [00:03, 1068.79it/s]\u001b[32m2025-02-13 13:28:04.527\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 1 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_1\u001b[0m\n",
      "Streaming from Hugging Face: 5810it [00:04, 1867.53it/s]\u001b[32m2025-02-13 13:28:05.758\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 2 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_2\u001b[0m\n",
      "Streaming from Hugging Face: 6729it [00:05, 1363.40it/s]\u001b[32m2025-02-13 13:28:07.080\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 3 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_3\u001b[0m\n",
      "Streaming from Hugging Face: 8459it [00:06, 1337.72it/s]\u001b[32m2025-02-13 13:28:08.791\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 4 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_4\u001b[0m\n",
      "Streaming from Hugging Face: 10970it [00:08, 1532.41it/s]\u001b[32m2025-02-13 13:28:10.161\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 5 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_5\u001b[0m\n",
      "Streaming from Hugging Face: 13659it [00:09, 1938.76it/s]\u001b[32m2025-02-13 13:28:11.379\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 6 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_6\u001b[0m\n",
      "Streaming from Hugging Face: 14431it [00:11, 1382.73it/s]\u001b[32m2025-02-13 13:28:12.442\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 7 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_7\u001b[0m\n",
      "Streaming from Hugging Face: 17524it [00:12, 2087.77it/s]\u001b[32m2025-02-13 13:28:13.621\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 8 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_8\u001b[0m\n",
      "Streaming from Hugging Face: 18288it [00:13, 1487.44it/s]\u001b[32m2025-02-13 13:28:14.713\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 9 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_9\u001b[0m\n",
      "Streaming from Hugging Face: 21588it [00:14, 2215.13it/s]\u001b[32m2025-02-13 13:28:15.823\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 10 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_10\u001b[0m\n",
      "Streaming from Hugging Face: 22371it [00:15, 1610.94it/s]\u001b[32m2025-02-13 13:28:16.988\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 11 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_11\u001b[0m\n",
      "Streaming from Hugging Face: 25396it [00:16, 2124.92it/s]\u001b[32m2025-02-13 13:28:18.362\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 12 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_12\u001b[0m\n",
      "Streaming from Hugging Face: 27941it [00:18, 2224.12it/s]\u001b[32m2025-02-13 13:28:19.571\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 13 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_13\u001b[0m\n",
      "Streaming from Hugging Face: 28855it [00:19, 1572.03it/s]\u001b[32m2025-02-13 13:28:20.772\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 14 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_14\u001b[0m\n",
      "Streaming from Hugging Face: 31351it [00:20, 1921.93it/s]\u001b[32m2025-02-13 13:28:21.955\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 15 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_15\u001b[0m\n",
      "Streaming from Hugging Face: 33770it [00:21, 2191.26it/s]\u001b[32m2025-02-13 13:28:23.013\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 16 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_16\u001b[0m\n",
      "Streaming from Hugging Face: 35409it [00:22, 1971.52it/s]\u001b[32m2025-02-13 13:28:24.313\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 17 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_17\u001b[0m\n",
      "Streaming from Hugging Face: 37821it [00:24, 2215.02it/s]\u001b[32m2025-02-13 13:28:25.386\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 18 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_18\u001b[0m\n",
      "Streaming from Hugging Face: 38686it [00:25, 1590.89it/s]\u001b[32m2025-02-13 13:28:26.441\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 19 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_19\u001b[0m\n",
      "Streaming from Hugging Face: 41861it [00:26, 2391.72it/s]\u001b[32m2025-02-13 13:28:27.492\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 20 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_20\u001b[0m\n",
      "Streaming from Hugging Face: 42756it [00:27, 1739.50it/s]\u001b[32m2025-02-13 13:28:28.476\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 21 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_21\u001b[0m\n",
      "Streaming from Hugging Face: 45788it [00:28, 2458.91it/s]\u001b[32m2025-02-13 13:28:29.532\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 22 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_22\u001b[0m\n",
      "Streaming from Hugging Face: 46673it [00:29, 1766.53it/s]\u001b[32m2025-02-13 13:28:30.549\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 23 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_23\u001b[0m\n",
      "Streaming from Hugging Face: 49130it [00:30, 2132.55it/s]\u001b[32m2025-02-13 13:28:31.532\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 24 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_24\u001b[0m\n",
      "Streaming from Hugging Face: 51282it [00:31, 2350.52it/s]\u001b[32m2025-02-13 13:28:32.705\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 25 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_25\u001b[0m\n",
      "Streaming from Hugging Face: 53737it [00:32, 2486.76it/s]\u001b[32m2025-02-13 13:28:33.717\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 26 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_26\u001b[0m\n",
      "Streaming from Hugging Face: 54629it [00:33, 1776.73it/s]\u001b[32m2025-02-13 13:28:34.699\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 27 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_27\u001b[0m\n",
      "Streaming from Hugging Face: 57351it [00:34, 2326.78it/s]\u001b[32m2025-02-13 13:28:35.705\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 28 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_28\u001b[0m\n",
      "Streaming from Hugging Face: 59051it [00:35, 2161.56it/s]\u001b[32m2025-02-13 13:28:36.798\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 29 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_29\u001b[0m\n",
      "Streaming from Hugging Face: 60826it [00:36, 2036.96it/s]\u001b[32m2025-02-13 13:28:37.826\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 30 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_30\u001b[0m\n",
      "Streaming from Hugging Face: 63751it [00:37, 2724.65it/s]\u001b[32m2025-02-13 13:28:38.811\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 31 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_31\u001b[0m\n",
      "Streaming from Hugging Face: 64596it [00:38, 1865.51it/s]\u001b[32m2025-02-13 13:28:39.832\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 32 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_32\u001b[0m\n",
      "Streaming from Hugging Face: 67743it [00:39, 2595.26it/s]\u001b[32m2025-02-13 13:28:40.851\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 33 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_33\u001b[0m\n",
      "Streaming from Hugging Face: 68600it [00:40, 1831.51it/s]\u001b[32m2025-02-13 13:28:41.842\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 34 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_34\u001b[0m\n",
      "Streaming from Hugging Face: 71469it [00:41, 2416.15it/s]\u001b[32m2025-02-13 13:28:42.827\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 35 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_35\u001b[0m\n",
      "Streaming from Hugging Face: 72248it [00:42, 1765.77it/s]\u001b[32m2025-02-13 13:28:43.822\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 36 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_36\u001b[0m\n",
      "Streaming from Hugging Face: 75482it [00:43, 2487.23it/s]\u001b[32m2025-02-13 13:28:44.867\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 37 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_37\u001b[0m\n",
      "Streaming from Hugging Face: 76245it [00:44, 1756.82it/s]\u001b[32m2025-02-13 13:28:45.915\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 38 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_38\u001b[0m\n",
      "Streaming from Hugging Face: 79368it [00:45, 2363.36it/s]\u001b[32m2025-02-13 13:28:46.927\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 39 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_39\u001b[0m\n",
      "Streaming from Hugging Face: 81974it [00:46, 2738.57it/s]\u001b[32m2025-02-13 13:28:47.942\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 40 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_40\u001b[0m\n",
      "Streaming from Hugging Face: 82909it [00:47, 1925.75it/s]\u001b[32m2025-02-13 13:28:48.918\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 41 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_41\u001b[0m\n",
      "Streaming from Hugging Face: 85185it [00:48, 2187.76it/s]\u001b[32m2025-02-13 13:28:49.947\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 42 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_42\u001b[0m\n",
      "Streaming from Hugging Face: 87414it [00:49, 2431.06it/s]\u001b[32m2025-02-13 13:28:50.966\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 43 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_43\u001b[0m\n",
      "Streaming from Hugging Face: 89410it [00:50, 2398.11it/s]\u001b[32m2025-02-13 13:28:51.997\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 44 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_44\u001b[0m\n",
      "Streaming from Hugging Face: 91455it [00:51, 2388.73it/s]\u001b[32m2025-02-13 13:28:53.032\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 45 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_45\u001b[0m\n",
      "Streaming from Hugging Face: 93994it [00:52, 2732.37it/s]\u001b[32m2025-02-13 13:28:54.016\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 46 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_46\u001b[0m\n",
      "Streaming from Hugging Face: 94921it [00:53, 1923.97it/s]\u001b[32m2025-02-13 13:28:55.012\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 47 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_47\u001b[0m\n",
      "Streaming from Hugging Face: 97763it [00:54, 2542.34it/s]\u001b[32m2025-02-13 13:28:56.006\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 48 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_48\u001b[0m\n",
      "Streaming from Hugging Face: 98649it [00:55, 1838.84it/s]\u001b[32m2025-02-13 13:28:56.987\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 49 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_49\u001b[0m\n",
      "Streaming from Hugging Face: 101842it [00:56, 2617.29it/s]\u001b[32m2025-02-13 13:28:57.941\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 50 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_50\u001b[0m\n",
      "Streaming from Hugging Face: 102743it [00:57, 1925.38it/s]\u001b[32m2025-02-13 13:28:58.965\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 51 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_51\u001b[0m\n",
      "Streaming from Hugging Face: 105358it [00:58, 2334.74it/s]\u001b[32m2025-02-13 13:28:59.978\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 52 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_52\u001b[0m\n",
      "Streaming from Hugging Face: 107762it [00:59, 2587.77it/s]\u001b[32m2025-02-13 13:29:01.077\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 53 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_53\u001b[0m\n",
      "Streaming from Hugging Face: 108644it [01:00, 1771.83it/s]\u001b[32m2025-02-13 13:29:02.205\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 54 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_54\u001b[0m\n",
      "Streaming from Hugging Face: 111491it [01:01, 2254.31it/s]\u001b[32m2025-02-13 13:29:03.240\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 55 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_55\u001b[0m\n",
      "Streaming from Hugging Face: 112280it [01:02, 1646.83it/s]\u001b[32m2025-02-13 13:29:04.366\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 56 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_56\u001b[0m\n",
      "Streaming from Hugging Face: 115682it [01:04, 2383.86it/s]\u001b[32m2025-02-13 13:29:05.356\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 57 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_57\u001b[0m\n",
      "Streaming from Hugging Face: 116503it [01:05, 1785.91it/s]\u001b[32m2025-02-13 13:29:06.328\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 58 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_58\u001b[0m\n",
      "Streaming from Hugging Face: 119152it [01:06, 2236.35it/s]\u001b[32m2025-02-13 13:29:07.355\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 59 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_59\u001b[0m\n",
      "Streaming from Hugging Face: 121376it [01:07, 2411.22it/s]\u001b[32m2025-02-13 13:29:08.370\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 60 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_60\u001b[0m\n",
      "Streaming from Hugging Face: 123824it [01:08, 2663.34it/s]\u001b[32m2025-02-13 13:29:09.437\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 61 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_61\u001b[0m\n",
      "Streaming from Hugging Face: 124720it [01:09, 1830.79it/s]\u001b[32m2025-02-13 13:29:10.510\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 62 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_62\u001b[0m\n",
      "Streaming from Hugging Face: 127117it [01:10, 2108.46it/s]\u001b[32m2025-02-13 13:29:11.555\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 63 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_63\u001b[0m\n",
      "Streaming from Hugging Face: 129942it [01:11, 2683.70it/s]\u001b[32m2025-02-13 13:29:12.506\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 64 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_64\u001b[0m\n",
      "Streaming from Hugging Face: 130319it [01:12, 1805.96it/s]\n",
      "\u001b[32m2025-02-13 13:29:13.320\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 65 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/chunk_65\u001b[0m\n",
      "No changes to commit on branch 'transaction-713776'.\n",
      "\u001b[32m2025-02-13 13:29:13.435\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mstream_and_upload_from_hf_to_lakefs\u001b[0m:\u001b[36m59\u001b[0m - \u001b[32m\u001b[1mUploaded dataset from huggingface rajpurkar/squad_v2 to lakefs\u001b[0m\n",
      "Streaming from Hugging Face: 1385it [00:01, 1464.35it/s]\u001b[32m2025-02-13 13:29:17.857\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 0 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/validation/chunk_0\u001b[0m\n",
      "Streaming from Hugging Face: 3935it [00:02, 2531.28it/s]\u001b[32m2025-02-13 13:29:18.878\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 1 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/validation/chunk_1\u001b[0m\n",
      "Streaming from Hugging Face: 4914it [00:03, 1706.01it/s]\u001b[32m2025-02-13 13:29:19.860\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 2 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/validation/chunk_2\u001b[0m\n",
      "Streaming from Hugging Face: 6511it [00:04, 1705.75it/s]\u001b[32m2025-02-13 13:29:20.950\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 3 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/validation/chunk_3\u001b[0m\n",
      "Streaming from Hugging Face: 9312it [00:05, 2342.68it/s]\u001b[32m2025-02-13 13:29:21.998\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 4 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/validation/chunk_4\u001b[0m\n",
      "Streaming from Hugging Face: 11873it [00:06, 1845.84it/s]\n",
      "\u001b[32m2025-02-13 13:29:23.021\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mprocess_and_upload_chunk\u001b[0m:\u001b[36m24\u001b[0m - \u001b[32m\u001b[1mUploaded chunk 5 to lakefs://qa-manual/main/raw/rajpurkar_squad_v2/validation/chunk_5\u001b[0m\n",
      "No changes to commit on branch 'transaction-210670'.\n",
      "\u001b[32m2025-02-13 13:29:23.133\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdata.etl.hf_to_lakefs\u001b[0m:\u001b[36mstream_and_upload_from_hf_to_lakefs\u001b[0m:\u001b[36m59\u001b[0m - \u001b[32m\u001b[1mUploaded dataset from huggingface rajpurkar/squad_v2 to lakefs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from data.etl.hf_to_lakefs import stream_and_upload_from_hf_to_lakefs\n",
    "from data.utils.hugging_face import get_info\n",
    "\n",
    "info = get_info(hf_dataset_name)\n",
    "splits = info.splits.keys()\n",
    "for split in splits:\n",
    "  address = stream_and_upload_from_hf_to_lakefs(hf_dataset_name=hf_dataset_name,dataset=lakefs_dataset,split=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'main'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lakefs_dataset.lakefs_client.tx.commit(f\"Uploaded dataset from huggingface {hf_dataset_name} to lakefs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-05 16:54:32.721\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.utils.lakefs\u001b[0m:\u001b[36mls\u001b[0m:\u001b[36m223\u001b[0m - \u001b[1mListing files in lakefs://qa-manual/main/raw/rajpurkar_squad_v2/train/**/*.parquet\u001b[0m\n",
      "\u001b[32m2025-02-05 16:54:32.866\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdata.utils.lakefs\u001b[0m:\u001b[36mls\u001b[0m:\u001b[36m223\u001b[0m - \u001b[1mListing files in lakefs://qa-manual/main/raw/rajpurkar_squad_v2/validation/**/*.parquet\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': [], 'validation': []}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = lakefs_dataset.load_data_files()\n",
    "data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from IPython.display import display, HTML\n",
    "ipy_html = lambda df,h : display(HTML(f\"<h2>{h}</h2>\" + df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>train</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&amp;B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>{'text': ['in the late 1990s'], 'answer_start': [269]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&amp;B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".</td>\n",
       "      <td>What areas did Beyonce compete in when she was growing up?</td>\n",
       "      <td>{'text': ['singing and dancing'], 'answer_start': [207]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>validation</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>Normans</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.</td>\n",
       "      <td>In what country is Normandy located?</td>\n",
       "      <td>{'text': ['France', 'France', 'France', 'France'], 'answer_start': [159, 159, 159, 159]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56ddde6b9a695914005b9629</td>\n",
       "      <td>Normans</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.</td>\n",
       "      <td>When were the Normans in Normandy?</td>\n",
       "      <td>{'text': ['10th and 11th centuries', 'in the 10th and 11th centuries', '10th and 11th centuries', '10th and 11th centuries'], 'answer_start': [94, 87, 94, 94]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for title, path in data_files.items():\n",
    "    df = dd.read_parquet(path[0], columns=None, \n",
    "                          index=False, \n",
    "                          storage_options=None,\n",
    "                          engine='pyarrow', \n",
    "                          gather_statistics=False, \n",
    "                          split_row_groups=True,\n",
    "                          chunksize=10)\n",
    "    ipy_html(df.head(2),h=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.preprocess.bronze.dask_processes import ExplodeProcess,ExtractNestedProcess,DaskDataProcess , AddConversation\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def agregation_function(df: pd.DataFrame) -> List[np.ndarray]:\n",
    "    user_text = (df[\"question\"].astype(str) + df[\"context\"].astype(str)).to_numpy()\n",
    "    assistant_text = df[\"text\"].astype(str).to_numpy()\n",
    "    assistant_text = np.where((assistant_text != \"\") & (pd.Series(assistant_text).str.len() > 2).to_numpy(), \n",
    "                              \"the answer to your question based on provided knowledge is \" + assistant_text, \n",
    "                              assistant_text)\n",
    "    assistant_text = np.where((assistant_text == \" \") | (pd.Series(assistant_text).str.len() <= 2).to_numpy(), \n",
    "                              \"sorry, I don't know the answer to your question\", \n",
    "                              assistant_text)\n",
    "    return user_text, assistant_text\n",
    "\n",
    "\n",
    "get_text_from_answer = ExtractNestedProcess(new_expected_columns={\"text\": \"object\"}, nested_column=\"answers\")(df)\n",
    "get_text_from_array_text = ExplodeProcess(new_expected_columns={\"text\": \"string\"})(meta=get_text_from_answer[\"meta\"])\n",
    "get_human = AddConversation(agregation_function=agregation_function)(meta=get_text_from_array_text[\"meta\"])\n",
    "new_df = df .map_partitions(**get_text_from_answer)\\\n",
    "            .map_partitions(**get_text_from_array_text)\\\n",
    "            .map_partitions(**get_human)\\\n",
    "            .map_partitions(lambda df: df[[\"conversation\"]])\n",
    "\n",
    "a = new_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert partition to expected pyarrow schema:\n    `ArrowTypeError(\"Expected bytes, got a 'list' object\", 'Conversion failed for column conversation with type object')`\n\nExpected partition schema:\n    conversation: large_string\n    __null_dask_index__: int64\n\nReceived partition schema:\n    conversation: list<item: struct<content: string, role: string>>\n      child 0, item: struct<content: string, role: string>\n          child 0, content: string\n          child 1, role: string\n    __null_dask_index__: int64\n\nThis error *may* be resolved by passing in schema information for\nthe mismatched column(s) using the `schema` keyword in `to_parquet`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnew_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/lmorbits/packages/data/tmp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_metadata_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desk/projects/university/slmops-project/slmops-thesis/lmorbits/.venv/lib/python3.12/site-packages/dask_expr/_collection.py:3296\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m   3293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_parquet\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3294\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask_expr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 3296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desk/projects/university/slmops-project/slmops-thesis/lmorbits/.venv/lib/python3.12/site-packages/dask_expr/io/parquet.py:653\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, compression, write_index, append, overwrite, ignore_divisions, partition_on, storage_options, custom_metadata, write_metadata_file, compute, compute_kwargs, schema, name_function, filesystem, engine, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m     out \u001b[38;5;241m=\u001b[39m new_collection(\n\u001b[1;32m    633\u001b[0m         ToParquet(\n\u001b[1;32m    634\u001b[0m             df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n\u001b[1;32m    650\u001b[0m     )\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute:\n\u001b[0;32m--> 653\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcompute_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;66;03m# Invalidate the filesystem listing cache for the output path after write.\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;66;03m# We do this before returning, even if `compute=False`. This helps ensure\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# that reading files that were just written succeeds.\u001b[39;00m\n\u001b[1;32m    658\u001b[0m fs\u001b[38;5;241m.\u001b[39minvalidate_cache(path)\n",
      "File \u001b[0;32m~/Desk/projects/university/slmops-project/slmops-thesis/lmorbits/.venv/lib/python3.12/site-packages/dask_expr/_collection.py:477\u001b[0m, in \u001b[0;36mFrameBase.compute\u001b[0;34m(self, fuse, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mrepartition(npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    476\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39moptimize(fuse\u001b[38;5;241m=\u001b[39mfuse)\n\u001b[0;32m--> 477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDaskMethodsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desk/projects/university/slmops-project/slmops-thesis/lmorbits/.venv/lib/python3.12/site-packages/dask/base.py:376\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    353\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Desk/projects/university/slmops-project/slmops-thesis/lmorbits/.venv/lib/python3.12/site-packages/dask/base.py:662\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 662\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/Desk/projects/university/slmops-project/slmops-thesis/lmorbits/.venv/lib/python3.12/site-packages/dask_expr/_expr.py:3758\u001b[0m, in \u001b[0;36mFused._execute_task\u001b[0;34m(graph, name, *deps)\u001b[0m\n\u001b[1;32m   3756\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, dep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(deps):\n\u001b[1;32m   3757\u001b[0m     graph[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;241m=\u001b[39m dep\n\u001b[0;32m-> 3758\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desk/projects/university/slmops-project/slmops-thesis/lmorbits/.venv/lib/python3.12/site-packages/dask/dataframe/io/parquet/core.py:171\u001b[0m, in \u001b[0;36mToParquetFunctionWrapper.__call__\u001b[0;34m(self, df, block_index)\u001b[0m\n\u001b[1;32m    164\u001b[0m filename \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpart.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpart_i\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi_offset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_function(part_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi_offset)\n\u001b[1;32m    168\u001b[0m )\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Write out data\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_partition\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartition_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_metadata_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart_i\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs_pass\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desk/projects/university/slmops-project/slmops-thesis/lmorbits/.venv/lib/python3.12/site-packages/dask/dataframe/io/parquet/arrow.py:894\u001b[0m, in \u001b[0;36mArrowDatasetEngine.write_partition\u001b[0;34m(cls, df, path, fs, filename, partition_on, return_metadata, fmd, compression, index_cols, schema, head, custom_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m     index_cols \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 894\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pandas_to_arrow_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m custom_metadata:\n\u001b[1;32m    896\u001b[0m     _md \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mmetadata\n",
      "File \u001b[0;32m~/Desk/projects/university/slmops-project/slmops-thesis/lmorbits/.venv/lib/python3.12/site-packages/dask/dataframe/io/parquet/arrow.py:855\u001b[0m, in \u001b[0;36mArrowDatasetEngine._pandas_to_arrow_table\u001b[0;34m(cls, df, preserve_index, schema)\u001b[0m\n\u001b[1;32m    849\u001b[0m expected \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mindent(\n\u001b[1;32m    850\u001b[0m     schema\u001b[38;5;241m.\u001b[39mto_string(show_schema_metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    851\u001b[0m )\n\u001b[1;32m    852\u001b[0m actual \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mindent(\n\u001b[1;32m    853\u001b[0m     df_schema\u001b[38;5;241m.\u001b[39mto_string(show_schema_metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    854\u001b[0m )\n\u001b[0;32m--> 855\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to convert partition to expected pyarrow schema:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected partition schema:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived partition schema:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis error *may* be resolved by passing in schema information for\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe mismatched column(s) using the `schema` keyword in `to_parquet`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert partition to expected pyarrow schema:\n    `ArrowTypeError(\"Expected bytes, got a 'list' object\", 'Conversion failed for column conversation with type object')`\n\nExpected partition schema:\n    conversation: large_string\n    __null_dask_index__: int64\n\nReceived partition schema:\n    conversation: list<item: struct<content: string, role: string>>\n      child 0, item: struct<content: string, role: string>\n          child 0, content: string\n          child 1, role: string\n    __null_dask_index__: int64\n\nThis error *may* be resolved by passing in schema information for\nthe mismatched column(s) using the `schema` keyword in `to_parquet`."
     ]
    }
   ],
   "source": [
    "new_df.to_parquet(\n",
    "    \"/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/lmorbits/packages/data/tmp\",\n",
    "    engine=\"pyarrow\",\n",
    "    write_metadata_file=True,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lakefs://qa-manual/main/bronze/bronze/bronze/bronze/bronze/bronze/rajpurkar_squad_v2'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lakefs_dataset_bronze.get_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LakeFSCredentials(endpoint_url='https://lakefs.lmorbits.com', access_key_id='AKIAJ6WINMOUXFQBDZSQ', secret_access_key='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', namespace='gs://slmops-dev-data-instructed')\n",
      "Found existing repo qa-manual using storage namespace gs://slmops-dev-data-instructed/lakefs/qa-manual\n",
      "Found existing branch main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No changes to commit on branch 'transaction-911954'.\n",
      "\u001b[32m2025-02-05 16:47:52.619\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m45\u001b[0m - \u001b[32m\u001b[1mUploaded dataset from huggingface rajpurkar/squad_v2 to lakefs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "import pyarrow as pa\n",
    "\n",
    "lakefs_dataset_bronze = LakeFsDataset(credentials=credentials,\n",
    "                               dataset_type=DatasetType(\"bronze\"), \n",
    "                               directory=directory, \n",
    "                               project_name=project_name, \n",
    "                              )\n",
    "\n",
    "lakefs_client = lakefs_dataset_bronze.lakefs_client\n",
    "\n",
    "with lakefs_client.fs.transaction(lakefs_client.repo_manager.repo_name, lakefs_client.branch_manager.current_branch) as tx:\n",
    "    get_text_from_answer = ExtractNestedProcess(new_expected_columns={\"text\": \"object\"}, nested_column=\"answers\")(df)\n",
    "    get_text_from_array_text = ExplodeProcess(new_expected_columns={\"text\": \"string\"})(meta=get_text_from_answer[\"meta\"])\n",
    "    get_human = AddConversation(agregation_function=agregation_function)(meta=get_text_from_array_text[\"meta\"])\n",
    "    new_df = df .map_partitions(**get_text_from_answer)\\\n",
    "                .map_partitions(**get_text_from_array_text)\\\n",
    "                .map_partitions(**get_human)\\\n",
    "                .map_partitions(lambda df: df[[\"conversation\"]])\n",
    "    \n",
    "    directory = lakefs_dataset_bronze.dataset.get_path()\n",
    "    path = f\"{lakefs_client.path}/{directory}/test\"\n",
    "    # Define the schema for the DataFrame\n",
    "    schema = pa.schema([\n",
    "    (\n",
    "        \"conversation\",\n",
    "        pa.list_(\n",
    "            pa.struct([\n",
    "                (\"content\", pa.string()),\n",
    "                (\"role\", pa.string())\n",
    "            ])\n",
    "        )\n",
    "    ),\n",
    "    (\"__null_dask_index__\", pa.int64())\n",
    "    ])\n",
    "    \n",
    "    new_df.to_parquet(\n",
    "        path,\n",
    "        engine=\"pyarrow\",\n",
    "        write_metadata_file=True,\n",
    "        filesystem=lakefs_client.fs,\n",
    "        overwrite=True,\n",
    "        schema=schema\n",
    "    )\n",
    "    tx.commit(f\"Uploaded dataset from huggingface {hf_dataset_name} to lakefs.\")\n",
    "    logger.success(f\"Uploaded dataset from huggingface {hf_dataset_name} to lakefs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the DataFrame\n",
    "schema = {\n",
    "    \"conversation\": \"list<item: struct<content: string, role: string>>\",\n",
    "    \"__null_dask_index__\": \"int64\"\n",
    "}\n",
    "\n",
    "# Use the schema in the to_parquet method\n",
    "new_df.to_parquet(\n",
    "    path,\n",
    "    engine=\"pyarrow\",\n",
    "    write_metadata_file=True,\n",
    "    filesystem=lakefs_client.fs,\n",
    "    overwrite=True,\n",
    "    schema=schema\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>bronze</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'role': 'user', 'content': 'In what country is Normandy located?The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.'}, {'role': 'assistant', 'content': 'the answer to your question based on provided knowledge is France'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'role': 'user', 'content': 'In what country is Normandy located?The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.'}, {'role': 'assistant', 'content': 'the answer to your question based on provided knowledge is France'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ipy_html(new_df.head(2),\"bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>Normans</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>In what country is Normandy located?</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56ddde6b9a695914005b9628</td>\n",
       "      <td>Normans</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>In what country is Normandy located?</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id    title  \\\n",
       "0  56ddde6b9a695914005b9628  Normans   \n",
       "0  56ddde6b9a695914005b9628  Normans   \n",
       "\n",
       "                                             context  \\\n",
       "0  The Normans (Norman: Nourmands; French: Norman...   \n",
       "0  The Normans (Norman: Nourmands; French: Norman...   \n",
       "\n",
       "                               question    text  \n",
       "0  In what country is Normandy located?  France  \n",
       "0  In what country is Normandy located?  France  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
